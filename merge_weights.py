import torch
from safetensors.torch import load_file, save_file
import argparse
import os
from tqdm import tqdm

def merge_models(base_path, ft_path, output_path):
    print(f"ğŸ”„ å¼€å§‹åˆå¹¶æµç¨‹...")
    print(f"1. åŠ è½½å®˜æ–¹åº•åº§æƒé‡ (Base): {base_path}")
    # åŠ è½½åˆ° CPU ä»¥èŠ‚çœæ˜¾å­˜ï¼Œé€šå¸¸ç³»ç»Ÿå†…å­˜è¶³å¤Ÿå¤„ç† 7B æ¨¡å‹
    base_state_dict = load_file(base_path, device="cpu")
    
    print(f"2. åŠ è½½å¾®è°ƒæƒé‡ (Fine-tuned): {ft_path}")
    ft_state_dict = load_file(ft_path, device="cpu")

    # å‡†å¤‡åˆå¹¶
    merged_state_dict = base_state_dict  # æµ…æ‹·è´ï¼Œç›´æ¥åœ¨ base ä¸Šä¿®æ”¹
    
    updated_keys = 0
    missing_in_base = []

    print("3. å¼€å§‹åˆå¹¶ä¸ç²¾åº¦è½¬æ¢ (Float32 -> BFloat16)...")
    
    # éå†å¾®è°ƒæ¨¡å‹çš„æ¯ä¸€ä¸ªå‚æ•°
    for key, value in tqdm(ft_state_dict.items(), desc="Updating weights"):
        # 1. å¤„ç† DDP è®­ç»ƒå¯èƒ½äº§ç”Ÿçš„ 'module.' å‰ç¼€
        clean_key = key.replace("module.", "")
        
        # 2. æ£€æŸ¥è¿™ä¸ª key æ˜¯å¦åœ¨åº•åº§æ¨¡å‹ä¸­å­˜åœ¨
        if clean_key in merged_state_dict:
            # 3. æ ¸å¿ƒæ­¥éª¤ï¼š
            #    a. å–å‡ºå¾®è°ƒçš„æƒé‡ (value)
            #    b. è½¬æ¢ä¸º bfloat16 (åŒ¹é…å®˜æ–¹æ ¼å¼)
            #    c. è¦†ç›–åº•åº§æ¨¡å‹çš„æƒé‡
            merged_state_dict[clean_key] = value.to(dtype=torch.bfloat16)
            updated_keys += 1
        else:
            # è¿™ç§æƒ…å†µæå°‘å‘ç”Ÿï¼Œé™¤éä½ è®­ç»ƒæ—¶ä¿®æ”¹äº†æ¨¡å‹ç»“æ„
            missing_in_base.append(clean_key)
            # ä¾ç„¶åŠ å…¥å­—å…¸ï¼Œä»¥é˜²ä¸‡ä¸€
            merged_state_dict[clean_key] = value.to(dtype=torch.bfloat16)

    print(f"\nâœ… åˆå¹¶å®Œæˆç»Ÿè®¡:")
    print(f"   - æ€»å…±æ›´æ–°äº† {updated_keys} ä¸ªå‚æ•°é”®å€¼ã€‚")
    print(f"   - Base ä¸­ä¿ç•™äº† {len(base_state_dict) - updated_keys} ä¸ªåŸæœ‰å‚æ•° (åŒ…æ‹¬ ViT/VAE ç­‰å†»ç»“å±‚)ã€‚")
    
    if missing_in_base:
        print(f"   - âš ï¸ è­¦å‘Š: æœ‰ {len(missing_in_base)} ä¸ªé”®åœ¨ Base ä¸­æœªæ‰¾åˆ° (å¯èƒ½æ˜¯æ–°å±‚): {missing_in_base[:5]}...")

    print(f"4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    # ä¿å­˜æ—¶æ·»åŠ  metadata æ ‡è®°ä¸º pt æ ¼å¼ï¼Œé˜²æ­¢è¯»å–æ—¶çš„ warning
    save_file(merged_state_dict, output_path, metadata={"format": "pt"})
    print("ğŸ‰ æ‰€æœ‰å·¥ä½œå®Œæˆï¼ç°åœ¨å¯ä»¥ç”¨è¿™ä¸ªæ–‡ä»¶è¿›è¡Œæ¨ç†äº†ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Merge finetuned partial weights with official base weights.")
    
    # ä½ çš„å®˜æ–¹ ema.safetensors è·¯å¾„ (ä¹Ÿå°±æ˜¯ä½ ä¸‹è½½çš„é‚£ä¸ªå®Œæ•´æ¨¡å‹)
    parser.add_argument("--base_model", type=str, required=True, help="Path to the official base ema.safetensors")
    
    # ä½ è®­ç»ƒå‡ºæ¥çš„ checkpoints/.../ema.safetensors
    parser.add_argument("--ft_model", type=str, required=True, help="Path to your finetuned ema.safetensors")
    
    # è¾“å‡ºè·¯å¾„
    parser.add_argument("--output", type=str, default="ema_merged.safetensors", help="Path to save the merged model")
    
    args = parser.parse_args()
    
    merge_models(args.base_model, args.ft_model, args.output)